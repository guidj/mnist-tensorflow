{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 05: ConvNet CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf        \n",
    "# to visualize the resutls\n",
    "import matplotlib.pyplot as plt \n",
    "# 70k mnist dataset that comes with the tensorflow container\n",
    "from tensorflow.examples.tutorials.mnist import input_data      \n",
    "\n",
    "tf.set_random_seed(0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(values, size):\n",
    "    b = np.zeros((values.shape[0], size))\n",
    "    b[np.arange(values.shape[0]), values] = 1\n",
    "    return b\n",
    "\n",
    "with open('data/cifar-10-batches-py/test_batch') as fp:\n",
    "    CF10_test = cPickle.load(fp)\n",
    "    \n",
    "CF10_X_test = np.array(CF10_test['data'])\n",
    "CF10_Y_test = np.array(CF10_test['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/cifar-10-batches-py/data_batch_1\n",
      "data/cifar-10-batches-py/data_batch_2\n",
      "data/cifar-10-batches-py/data_batch_5\n",
      "data/cifar-10-batches-py/data_batch_3\n",
      "data/cifar-10-batches-py/data_batch_4\n",
      "X_train: (50000, 3072), Y_train: (50000, 10)\n",
      "X_test: (10000, 3072), Y_test: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "path = 'data/cifar-10-batches-py/'\n",
    "CF10_X = None\n",
    "CF10_Y = None\n",
    "\n",
    "for filename in [os.path.join(path, f) for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]:\n",
    "    if 'data_batch' in filename:\n",
    "        with open(filename, 'r') as fp:\n",
    "            print(filename)\n",
    "            d = cPickle.load(fp)\n",
    "            if CF10_X is not None:\n",
    "                CF10_X = np.concatenate((CF10_X, d['data']), axis=0)\n",
    "                CF10_Y = np.concatenate((CF10_Y, d['labels']), axis=0)\n",
    "            else:\n",
    "                CF10_X = d['data']\n",
    "                CF10_Y = d['labels']\n",
    "\n",
    "CF10_Y = one_hot_encode(CF10_Y, 10)\n",
    "CF10_Y_test = one_hot_encode(CF10_Y_test, 10)\n",
    "\n",
    "print \"X_train: %s, Y_train: %s\" % (CF10_X.shape, CF10_Y.shape)\n",
    "print \"X_test: %s, Y_test: %s\" % (CF10_X_test.shape, CF10_Y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randombatch(n):\n",
    "    rows = np.random.randint(CF10_X.shape[0], size=n)    \n",
    "    return CF10_X[rows, :], CF10_Y[rows, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, strides):\n",
    "    return tf.nn.conv2d(x, W, strides=strides, padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def runtest():\n",
    "    \n",
    "    acc, cc = [], []\n",
    "    for i in range(0, CF10_X_test.shape[0], 500):\n",
    "        rows = np.array(range(i, i+500))\n",
    "    \n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict={X: CF10_X_test[rows, :], Y_: CF10_Y_test[rows, :],  pkeep: 1.0})\n",
    "        acc.append(a)\n",
    "        cc.append(c)\n",
    "    \n",
    "    _accuracy, _loss = sum(acc)/len(acc), sum(cc)/len(cc)\n",
    "    return _accuracy, _loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 TrainA[0.110000], TrainC[2.351152]\n",
      "TestA[0.112200], TestC[2.346878]\n",
      "100 TrainA[0.140000], TrainC[2.321150]\n",
      "TestA[0.100000], TestC[2.361145]\n",
      "200 TrainA[0.070000], TrainC[2.391150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "300 TrainA[0.160000], TrainC[2.299074]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "400 TrainA[0.150000], TrainC[2.311101]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "500 TrainA[0.120000], TrainC[2.341150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "600 TrainA[0.120000], TrainC[2.341150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "700 TrainA[0.100000], TrainC[2.361150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "800 TrainA[0.090000], TrainC[2.371150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "900 TrainA[0.080000], TrainC[2.381150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "1000 TrainA[0.130000], TrainC[2.331150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "1100 TrainA[0.080000], TrainC[2.381150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "1200 TrainA[0.080000], TrainC[2.381150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "1300 TrainA[0.140000], TrainC[2.321150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "1400 TrainA[0.080000], TrainC[2.381150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "1500 TrainA[0.080000], TrainC[2.381150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "1600 TrainA[0.110000], TrainC[2.351150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "1700 TrainA[0.090000], TrainC[2.371095]\n",
      "TestA[0.100300], TestC[2.360850]\n",
      "1800 TrainA[0.140000], TrainC[2.321040]\n",
      "TestA[0.103600], TestC[2.357500]\n",
      "1900 TrainA[0.080000], TrainC[2.381150]\n",
      "TestA[0.100100], TestC[2.361051]\n",
      "2000 TrainA[0.100000], TrainC[2.361150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "2100 TrainA[0.110000], TrainC[2.351150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "2200 TrainA[0.110000], TrainC[2.351062]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "2300 TrainA[0.070000], TrainC[2.391150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "2400 TrainA[0.110000], TrainC[2.351150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "2500 TrainA[0.130000], TrainC[2.331150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "2600 TrainA[0.100000], TrainC[2.361150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "2700 TrainA[0.100000], TrainC[2.361150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "2800 TrainA[0.120000], TrainC[2.341150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "2900 TrainA[0.100000], TrainC[2.361150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "3000 TrainA[0.190000], TrainC[2.271150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "3100 TrainA[0.100000], TrainC[2.361149]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "3200 TrainA[0.070000], TrainC[2.391086]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "3300 TrainA[0.060000], TrainC[2.401150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "3400 TrainA[0.140000], TrainC[2.321150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "3500 TrainA[0.140000], TrainC[2.321150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "3600 TrainA[0.100000], TrainC[2.361150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "3700 TrainA[0.080000], TrainC[2.381150]\n",
      "TestA[0.100000], TestC[2.361150]\n",
      "3769"
     ]
    }
   ],
   "source": [
    "                                        \n",
    "# load data, 60K trainset and 10K testset\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)  \n",
    "\n",
    "# 1. Define Variables and Placeholders\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 3072])\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "XX = tf.reshape(X, [-1, 32, 32, 3])\n",
    "pkeep = tf.placeholder(tf.float32)\n",
    "\n",
    "# 2. Define the model\n",
    "\n",
    "# Stride of 2 reduces WxH by 2?\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.005\n",
    "lr = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           100000, 0.96, staircase=False)\n",
    "\n",
    "# we get one vector from one conv layer, and pass it to the next one to transform it\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 3, 12], stddev=0.1))\n",
    "B1 = tf.Variable(tf.zeros([12]))\n",
    "W1CONV = tf.nn.relu(conv2d(XX, W1, strides=[1,1,1,1]) + B1)\n",
    "W1CONVD = tf.nn.dropout(W1CONV, pkeep)\n",
    "\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, 12, 8], stddev=0.1))\n",
    "B2 = tf.Variable(tf.zeros([8]))\n",
    "W2CONV = tf.nn.relu(conv2d(W1CONVD, W2, strides=[1,2,2,1]) + B2)\n",
    "W2CONVD = tf.nn.dropout(W2CONV, pkeep)\n",
    "\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([5, 5, 8, 12], stddev=0.1))\n",
    "B3 = tf.Variable(tf.zeros([12]))\n",
    "W3CONV = tf.nn.relu(conv2d(W2CONVD, W3, strides=[1,2,2,1]) + B3)\n",
    "W3CONVD = tf.nn.dropout(W3CONV, pkeep)\n",
    "\n",
    "# print W1CONV.get_shape()\n",
    "# print W2CONV.get_shape()\n",
    "# print W3CONV.get_shape()\n",
    "\n",
    "W4 = weight_variable([8 * 8 * 12, 200])\n",
    "B4 = tf.Variable(tf.zeros([200]))\n",
    "W4FC = tf.nn.relu(tf.matmul(tf.reshape(W3CONVD, [-1, 8 * 8 * 12]), W4) + B4)\n",
    "W4FC = tf.nn.dropout(W4FC, pkeep)\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "Ylogits = tf.nn.relu(tf.matmul(W4FC, W5) + B5)\n",
    "Y = tf.nn.softmax(Ylogits)\n",
    "\n",
    "# # 3. Define the loss function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(Y, Y_))\n",
    "\n",
    "# # 4. Define the accuracy \n",
    "accuracy = tf.Variable(tf.zeros([1], tf.float32))\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# # 5. Define an optimizer\n",
    "# train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy, global_step=global_step)\n",
    "train_step = tf.train.MomentumOptimizer(lr, 10e-8).minimize(cross_entropy, global_step=global_step)\n",
    "\n",
    "# # initialize\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "\n",
    "def training_step(i, update_test_data, update_train_data):\n",
    "\n",
    "    print \"\\r\", i,\n",
    "    ####### actual learning \n",
    "    # reading batches of 100 images with 100 labels\n",
    "#     batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    batch_X, batch_Y = randombatch(100)\n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y,  pkeep: .6})\n",
    "    \n",
    "    ####### evaluating model performance for printing purposes\n",
    "    # evaluation used to later visualize how well you did at a particular time in the training\n",
    "    train_a = []\n",
    "    train_c = []\n",
    "    test_a = []\n",
    "    test_c = []\n",
    "    if update_train_data:\n",
    "        a, c = sess.run([accuracy, cross_entropy], feed_dict={X: batch_X, Y_: batch_Y, pkeep: .6})\n",
    "        train_a.append(a)\n",
    "        train_c.append(c)\n",
    "        print \"TrainA[%f], TrainC[%f]\" % (a, c)\n",
    "\n",
    "    if update_test_data:\n",
    "#         a, c = sess.run([accuracy, cross_entropy], feed_dict={X: CF10_X_test, Y_: CF10_Y_test,  pkeep: 1.0})\n",
    "        a, c = runtest()\n",
    "        test_a.append(a)\n",
    "        test_c.append(c)\n",
    "        print \"TestA[%f], TestC[%f]\" % (a, c)\n",
    "\n",
    "    \n",
    "    return (train_a, train_c, test_a, test_c)\n",
    "\n",
    "\n",
    "# 6. Train and test the model, store the accuracy and loss per iteration\n",
    "\n",
    "train_a = []\n",
    "train_c = []\n",
    "test_a = []\n",
    "test_c = []\n",
    "    \n",
    "training_iter = 10000\n",
    "epoch_size = 100\n",
    "for i in range(training_iter):\n",
    "    test = False\n",
    "    if i % epoch_size == 0:\n",
    "        test = True\n",
    "    a, c, ta, tc = training_step(i, test, test)\n",
    "    \n",
    "    train_a += a\n",
    "    train_c += c\n",
    "    test_a += ta\n",
    "    test_c += tc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7. Plot and visualise the accuracy and loss\n",
    "\n",
    "# accuracy training vs testing dataset\n",
    "plt.plot(train_a, label=\"Train A\")\n",
    "plt.plot(test_a, label=\"Test A\")\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(0, 1), loc='lower center', ncol=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loss training vs testing dataset\n",
    "plt.plot(train_c, label=\"Train C\")\n",
    "plt.plot(test_c, label=\"Test C\")\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(0, 1), loc='lower center', ncol=1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Zoom in on the tail of the plots\n",
    "zoom_point = 50\n",
    "x_range = range(zoom_point,training_iter/epoch_size)\n",
    "plt.plot(x_range, train_a[zoom_point:], label=\"Train A\")\n",
    "plt.plot(x_range, test_a[zoom_point:], label=\"Test A\")\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(0, 1), loc='lower center', ncol=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x_range, train_c[zoom_point:], label=\"Train C\")\n",
    "plt.plot(x_range, test_c[zoom_point:], label=\"Test C\")\n",
    "plt.grid(True)\n",
    "plt.legend(bbox_to_anchor=(0, 1), loc='lower center', ncol=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# a, c = sess.run([accuracy, cross_entropy], feed_dict={X: CF10_X_test, Y_: CF10_Y_test,  pkeep: 1.0})\n",
    "a, c = runtest()\n",
    "print \"Accuracy: %f, Loss: %f\" %(a, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing performs better than training. Best results with dropout .6-.65 instead of .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
